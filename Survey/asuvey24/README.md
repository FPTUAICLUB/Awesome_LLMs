# Large Language Models: A Survey 

![The Struture of Paper](./Image/Screenshot%20from%202024-05-23%2013-15-29.png)

The paper structure. 

## 1. Large Language Models 

### 1.1. Early Pre-trained Neural Language Models

Some key points of the development: 
n-gram -> RNNs -> LSTMs ...
we call it like Neural Language Models 
-> Important points: Transformer architecture 
-> Pretrained language model (PLMs) fine-tuned for many downstream tasks. 

#### 1.1.1. Encoder-only PLMs; 

Using oly encoder network 
->  developed for language understanding tasks, such as text classification, where the models need to predict a class label for an input text. 
Ex: Bert, RoBERTa, 

###